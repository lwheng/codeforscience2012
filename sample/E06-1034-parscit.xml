<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Estimating Markov model structures.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP-96,</booktitle>
<pages>893--896</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13692" citStr="Brants (1996)" startWordPosition="2267" endWordPosition="2268">ype b. A lot of people think 0 I will give away/RP the store (4) a. Saturday ’s crash ... that *T* killed 132 of the 146 people aboard/RB b. These are used * aboard/IN military helicopters Although not every ambiguity class is so cleanly delineated, this example demonstrates that such classes can be used to redefine a tagging model with more unified groupings. 5.1 Using complex ambiguity tags We thus propose splitting a class such as RB into subclasses, using these ambiguity classes—JJ/RB, NN/RB, IN/RB, etc.—akin to previous work on splitting labels in order to obtain better statistics (e.g., Brants (1996); Ule (2003)) for situations with “the same label but different usage” (Ule, 2003, p. 181). By taking this approach, we are narrowing in on what annotators were instructed to focus on, namely “difficult tagging decisions,” (Santorini, 1990, p. 7). We implement this idea by assigning words a new, complex tag composed of its ambiguity class and the benchmark tag for that position. For example, ago has the ambiguity class IN/RB, and in example (5a), it resolves to RB. Thus, following the notation in Pla and Molina (2004), we assign ago the complex ambiguity tag &lt;IN/RB,RB&gt; in the training data, as</context>
</contexts>
<marker>Brants, 1996</marker>
<rawString>Thorsten Brants. 1996. Estimating Markov model structures. In Proceedings of ICSLP-96, pages 893– 896, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP-2000,</booktitle>
<pages>224--231</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="8716" citStr="Brants, 2000" startWordPosition="1430" endWordPosition="1431">ent patterns in the corpus, it will then generalize these patterns to the problematic parts of the corpus. This approach hinges on high-quality error detection since in general we cannot assume that discrepancies between a POS tagger and the benchmark are errors in the benchmark. Van Halteren (2000), for example, found that his tagger was correct in only 20% of disagreements with the benchmark. By focusing only on the variationflagged positions, we expect the tagger decisions to be more often correct than incorrect. We use two off-the-shelf taggers for correction, the Markov model tagger TnT (Brants, 2000) and the Decision Tree Tagger (Schmid, 1997), which we will abbreviate as DTT. Both taggers use probabilistic contextual and lexical information to disambiguate a tag at a particular corpus position. The difference is that TnT obtains contextual probabilities from maximum likelihood counts, whereas DTT constructs binary-branching decision trees to obtain contextual probabilities. In both cases, instead of looking at n-grams of words, the taggers use n-grams of tags. This generalization is desirable, as the variation n-gram method shows that the corpus has conflicting labels for the exact same </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-ofspeech tagger. In Proceedings ofANLP-2000, pages 224–231, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Mihai Pop</author>
</authors>
<title>Unsupervised learning of disambiguation rules for part of speech tagging.</title>
<date>1999</date>
<booktitle>Natural Language Processing Using Very Large Corpora,</booktitle>
<pages>27--42</pages>
<editor>In Kenneth W. Church, editor,</editor>
<publisher>Kluwer Academic Press,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="19027" citStr="Brill and Pop (1999)" startWordPosition="3172" endWordPosition="3175">) a complex tag, if the word’s ambiguity tag also appears as a variation ambiguity; or (b) a simple tag, otherwise. Variation words (choice 1) We start with variation nuclei because these are the potential errors we wish to correct. An example of choice 1a is ago, which varies between IN and RB as a nucleus, and so receives the tag &lt;IN/RB,IN&gt; when it resolves to IN and &lt;IN/RB,RB&gt; when it resolves to RB. The choices are based on relevance, though; instead of simply assigning all tags occurring in an ambiguity to an ambiguity class, we filter out ambiguities which we deem irrelevant. Similar to Brill and Pop (1999) and Schmid (1997), we do this by examining the variation unigrams and removing tags which occur less than 0.01 of the time for a word and less than 10 times overall. This eliminates variations like ,/DT where DT appears 4210 times for an, but the comma tag appears only once. Doing this means that an can now be grouped with other unambiguous determiners (DT). In addition to removing some erroneous classes, we gain generality and avoid data sparseness by using fewer ambiguity classes. This pruning also means that some variation words will receive tags which are not part of a variation, which is</context>
</contexts>
<marker>Brill, Pop, 1999</marker>
<rawString>Eric Brill and Mihai Pop. 1999. Unsupervised learning of disambiguation rules for part of speech tagging. In Kenneth W. Church, editor, Natural Language Processing Using Very Large Corpora, pages 27–42. Kluwer Academic Press, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings ofANLP-92,</booktitle>
<pages>133--140</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="17380" citStr="Cutting et al (1992)" startWordPosition="2879" endWordPosition="2882">out behaves differently than the rest of its class—but the altered contextual probabilities, unlike a lexicalized tagger, bring general IN/RB/RP class information to bear on this tagging situation. Combining the two, we get the correct tag RB at this position. Since variation errors are errors for words with prominent ambiguity classes, zeroing in on these ambiguity classes should provide more accurate probabilities. For this to work, however, we have to ensure that we have the most effective ambiguity class for every word. 5.2 Assigning complex ambiguity tags In the tagging literature (e.g., Cutting et al (1992)) an ambiguity class is often composed of the set of every possible tag for a word. For correction, using every possible tag for an ambiguity class will result in too many classes, for two reasons: 1) there are erroneous tags which should not be part of the ambiguity class, and 2) some classes are irrelevant for disambiguating variation positions. Guided by these considerations, we use the procedure below to assign complex ambiguity tags to all words in the corpus, based on whether a word is a non-fringe variation nucleus and thus flagged as a potential error by the variation n-gram method (ch</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings ofANLP-92, pages 133–140, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e D´ejean</author>
</authors>
<title>How to evaluate and compare tagsets? a proposal.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-00,</booktitle>
<location>Athens.</location>
<marker>D´ejean, 2000</marker>
<rawString>Herv´e D´ejean. 2000. How to evaluate and compare tagsets? a proposal. In Proceedings of LREC-00, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting errors in part-of-speech annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL-03,</booktitle>
<pages>107--114</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1517" citStr="Dickinson and Meurers, 2003" startWordPosition="221" endWordPosition="224"> linguistics, and as a source of data for theoretical linguists searching for relevant language patterns. However, they contain annotation errors, and such errors provide unreliable training and evaluation data, as has been previously shown (see ch. 1 of Dickinson (2005) and references therein). Improving the quality of linguistic annotation where possible is thus a key issue for the use of annotated corpora in computational and theoretical linguistics. Research has gone into automatically detecting annotation errors for part-of-speech annotation (van Halteren, 2000; Kvˇetˇon and Oliva, 2002; Dickinson and Meurers, 2003), yet there has been virtually no work on automatically or semiautomatically correcting such annotation errors.1 1Oliva (2001) specifies hand-written rules to detect and Automatic correction can speed up corpus improvement efforts and provide new data for NLP technology training on the corpus. Additionally, an investigation into automatic correction forces us to re-evaluate the technology using the corpus, providing new insights into such technology. We propose in this paper to automatically correct part-of-speech (POS) annotation errors in corpora, by adapting existing technology for POS disa</context>
<context position="3739" citStr="Dickinson and Meurers (2003)" startWordPosition="571" endWordPosition="574"> Automated annotation methods are not perfect, but humans also add errors, from biases and inconsistent judgments. Thus, automatic corpus correction methods can be used semi-automatically, just as the original corpus creation methods were used. then correct errors, but there is no general correction scheme. 265 2 Detecting POS Annotation Errors To correct part-of-speech (POS) annotation errors, one has to first detect such errors. Although there are POS error detection approaches, using, e.g., anomaly detection (Eskin, 2000), our approach builds on the variation n-gram algorithm introduced in Dickinson and Meurers (2003) and Dickinson (2005). As we will show in section 5, such a method is useful for correction because it highlights recurring problematic tag distinctions in the corpus. The idea behind the variation n-gram approach is that a string occurring more than once can occur with different labels in a corpus, which is referred to as variation. Variation is caused by one of two reasons: i) ambiguity: there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options, or ii) error: the tagging of a string is inconsistent across comparable </context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Markus Dickinson and W. Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In Proceedings of EACL-03, pages 107–114, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
</authors>
<title>Error detection and correction in annotated corpora.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="1160" citStr="Dickinson (2005)" startWordPosition="171" endWordPosition="172">ing process, we alter the tagging model to better account for problematic tagging distinctions. This modification results in significantly improved performance, reducing the error rate of the corpus. 1 Introduction Annotated corpora serve as training material and as “gold standard” testing material for the development of tools in computational linguistics, and as a source of data for theoretical linguists searching for relevant language patterns. However, they contain annotation errors, and such errors provide unreliable training and evaluation data, as has been previously shown (see ch. 1 of Dickinson (2005) and references therein). Improving the quality of linguistic annotation where possible is thus a key issue for the use of annotated corpora in computational and theoretical linguistics. Research has gone into automatically detecting annotation errors for part-of-speech annotation (van Halteren, 2000; Kvˇetˇon and Oliva, 2002; Dickinson and Meurers, 2003), yet there has been virtually no work on automatically or semiautomatically correcting such annotation errors.1 1Oliva (2001) specifies hand-written rules to detect and Automatic correction can speed up corpus improvement efforts and provide </context>
<context position="3760" citStr="Dickinson (2005)" startWordPosition="576" endWordPosition="578"> not perfect, but humans also add errors, from biases and inconsistent judgments. Thus, automatic corpus correction methods can be used semi-automatically, just as the original corpus creation methods were used. then correct errors, but there is no general correction scheme. 265 2 Detecting POS Annotation Errors To correct part-of-speech (POS) annotation errors, one has to first detect such errors. Although there are POS error detection approaches, using, e.g., anomaly detection (Eskin, 2000), our approach builds on the variation n-gram algorithm introduced in Dickinson and Meurers (2003) and Dickinson (2005). As we will show in section 5, such a method is useful for correction because it highlights recurring problematic tag distinctions in the corpus. The idea behind the variation n-gram approach is that a string occurring more than once can occur with different labels in a corpus, which is referred to as variation. Variation is caused by one of two reasons: i) ambiguity: there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options, or ii) error: the tagging of a string is inconsistent across comparable occurrences. The more</context>
</contexts>
<marker>Dickinson, 2005</marker>
<rawString>Markus Dickinson. 2005. Error detection and correction in annotated corpora. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleazar Eskin</author>
</authors>
<title>Automatic corpus correction with anomaly detection.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL00,</booktitle>
<pages>148--153</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="3641" citStr="Eskin, 2000" startWordPosition="558" endWordPosition="559">tomatic tools must be used sensibly at every stage in the corpus building process. Automated annotation methods are not perfect, but humans also add errors, from biases and inconsistent judgments. Thus, automatic corpus correction methods can be used semi-automatically, just as the original corpus creation methods were used. then correct errors, but there is no general correction scheme. 265 2 Detecting POS Annotation Errors To correct part-of-speech (POS) annotation errors, one has to first detect such errors. Although there are POS error detection approaches, using, e.g., anomaly detection (Eskin, 2000), our approach builds on the variation n-gram algorithm introduced in Dickinson and Meurers (2003) and Dickinson (2005). As we will show in section 5, such a method is useful for correction because it highlights recurring problematic tag distinctions in the corpus. The idea behind the variation n-gram approach is that a string occurring more than once can occur with different labels in a corpus, which is referred to as variation. Variation is caused by one of two reasons: i) ambiguity: there is a type of string with multiple possible labels and different corpus occurrences of that string reali</context>
</contexts>
<marker>Eskin, 2000</marker>
<rawString>Eleazar Eskin. 2000. Automatic corpus correction with anomaly detection. In Proceedings of NAACL00, pages 148–153, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Kvˇetˇon</author>
<author>Karel Oliva</author>
</authors>
<title>Achieving an almost correct PoS-tagged corpus.</title>
<date>2002</date>
<booktitle>In Text, Speech and Dialogue (TSD</booktitle>
<pages>19--26</pages>
<publisher>Springer.</publisher>
<location>Heidelberg.</location>
<marker>Kvˇetˇon, Oliva, 2002</marker>
<rawString>Pavel Kvˇetˇon and Karel Oliva. 2002. Achieving an almost correct PoS-tagged corpus. In Text, Speech and Dialogue (TSD 2002), pages 19–26, Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4881" citStr="Marcus et al., 1993" startWordPosition="770" endWordPosition="773">ions, or ii) error: the tagging of a string is inconsistent across comparable occurrences. The more similar the context of a variation, the more likely the variation is an error. In Dickinson and Meurers (2003), contexts are composed of words, and identity of the context is required. The term variation n-gram refers to an n-gram (of words) in a corpus that contains a string annotated differently in another occurrence of the same ngram in the corpus. The string exhibiting the variation is referred to as the variation nucleus. For example, in the WSJ corpus, part of the Penn Treebank 3 release (Marcus et al., 1993), the string in (1) is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition (IN), while in another it is tagged as a particle (RP). (1) to ward off a hostile takeover attempt by two European shipping concerns Once the variation n-grams for a corpus have been computed, heuristics are employed to classify the variations into errors and ambiguities. The most effective heuristic takes into account the fact that natural languages favor the use of local dependencies over non-local ones: nuclei found at the fringe of an n-gram are more likely t</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Marquez</author>
<author>Lluis Padro</author>
</authors>
<title>A flexible POS tagger using an automatically acquired language model.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-97,</booktitle>
<pages>238--245</pages>
<location>Madrid, Spain.</location>
<contexts>
<context position="26440" citStr="Marquez and Padro (1997)" startWordPosition="4455" endWordPosition="4458">Figure 1: Summary of results the original. DTT sees a parallel overall improvement, from 97.47% to 98.33%. Clearly, then, each complex ambiguity model is a closer fit to the original corpus. Whether this means it is an overall better POS tagging model is an open question. Remaining issues We have shown that we can improve the annotation of a corpus by using tagging models with complex ambiguity tags, but can we improve even further? To do so, there are several obstacles to overcome. First, some distinctions cannot be handled by an automated system without semantic or non-local information. As Marquez and Padro (1997) point out, distinctions such as that between JJ and VBN are essentially semantic distinctions without any structural basis. For example, in the phrase proposed offering, the reason that proposed should be VBN is that it indicates a specific event. Since our method uses no external semantic information, we have no way to know how to correct this.8 Other distinctions, such as the one between VBD and VBN, require some form of non-local knowledge in order to disambiguate because it depends on the presence or absence of an auxiliary verb, which can be arbitrarily far away. Secondly, sometimes the </context>
</contexts>
<marker>Marquez, Padro, 1997</marker>
<rawString>Lluis Marquez and Lluis Padro. 1997. A flexible POS tagger using an automatically acquired language model. In Proceedings of ACL-97, pages 238–245, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Marquez</author>
<author>Lluis Padro</author>
<author>Horacio Rodriguez</author>
</authors>
<title>A machine learning approach to POS tagging.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="14731" citStr="Marquez et al (2000)" startWordPosition="2439" endWordPosition="2442">ass IN/RB, and in example (5a), it resolves to RB. Thus, following the notation in Pla and Molina (2004), we assign ago the complex ambiguity tag &lt;IN/RB,RB&gt; in the training data, as shown in (5b). (5) a. ago/RB b. ago/&lt;IN/RB,RB&gt; Complex ambiguity tags can provide better distinctions than the unaltered tags. For example, words which vary between IN and RB and tagged as IN (e.g., ago, tagged &lt;IN/RB,IN&gt;) can ignore the contextual information that words varying between DT (determiner) and IN (e.g., that, tagged &lt;DT/IN,IN&gt;) provide. This proposal is in the spirit of a tagger like that described in Marquez et al (2000), which breaks the POS tagging problem into one problem for each ambiguity class, but because we alter the tagset here, different underlying tagging algorithms can be used. To take an example, consider the 5-gram revenue of about $ 370 as it is tagged by TnT. The 5-gram (at position 1344) in the WSJ is annotated as in (6). The tag for about is incorrect since “about when used to mean ’approximately’ should be tagged as an adverb (RB), rather than a preposition (IN)” (Santorini, 1990, p. 22). (6) revenue/NN of/IN about/IN $/$ 370/CD Between of and $, the word about varies between preposition (I</context>
</contexts>
<marker>Marquez, Padro, Rodriguez, 2000</marker>
<rawString>Lluis Marquez, Lluis Padro, and Horacio Rodriguez. 2000. A machine learning approach to POS tagging. Machine Learning, 39(1):59–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<tech>Psychometrika,</tech>
<pages>12--153</pages>
<contexts>
<context position="10256" citStr="McNemar, 1947" startWordPosition="1682" endWordPosition="1684">lts are not. As mentioned, tagger-benchmark disagreements are more commonly tagger errors, but we find the opposite for variation-flagged positions. Narrowing in on the positions which the tagger changed, we find a precision of 58.56% (65/111) for TnT and 65.59% (69/107) for DTT. As the goal of correction is to change tags with 100% accuracy, we place a priority in improving these figures. One likely reason that DTT outperforms TnT is 4Note, then, that some typical tagging issues, such as dealing with unknown words, are not an issue for us. 5All p-values in this paper are from McNemar’s Test (McNemar, 1947) for analyzing matched dichotomous data (i.e., a correct or incorrect score for each corpus position from both models). its more flexible context. For instance, in example (2)—which DTT correctly changes and TnT does not— to know that such should be changed from adjective (JJ) to pre-determiner (PDT), one only need look at the following determiner (DT) an, and that provides enough context to disambiguate. TnT uses a fixed context of trigrams, and so can be swayed by irrelevant tags—here, the previous tags—which DTT can in principle ignore.6 (2) Mr. Bush was n’t interested in such/JJ an informa</context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12:153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel Oliva</author>
</authors>
<title>The possibilities of automatic detection/correction of errors in tagged corpora: a pilot study on a German corpus.</title>
<date>2001</date>
<booktitle>In Text, Speech and Dialogue (TSD</booktitle>
<pages>39--46</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1643" citStr="Oliva (2001)" startWordPosition="241" endWordPosition="242">ors, and such errors provide unreliable training and evaluation data, as has been previously shown (see ch. 1 of Dickinson (2005) and references therein). Improving the quality of linguistic annotation where possible is thus a key issue for the use of annotated corpora in computational and theoretical linguistics. Research has gone into automatically detecting annotation errors for part-of-speech annotation (van Halteren, 2000; Kvˇetˇon and Oliva, 2002; Dickinson and Meurers, 2003), yet there has been virtually no work on automatically or semiautomatically correcting such annotation errors.1 1Oliva (2001) specifies hand-written rules to detect and Automatic correction can speed up corpus improvement efforts and provide new data for NLP technology training on the corpus. Additionally, an investigation into automatic correction forces us to re-evaluate the technology using the corpus, providing new insights into such technology. We propose in this paper to automatically correct part-of-speech (POS) annotation errors in corpora, by adapting existing technology for POS disambiguation. We build the correction work on top of a POS error detection phase, described in section 2. In section 3 we discus</context>
</contexts>
<marker>Oliva, 2001</marker>
<rawString>Karel Oliva. 2001. The possibilities of automatic detection/correction of errors in tagged corpora: a pilot study on a German corpus. In Text, Speech and Dialogue (TSD 2001), pages 39–46. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferran Pla</author>
<author>Antonio Molina</author>
</authors>
<title>Improving partof-speech tagging using lexicalized HMMs.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="14215" citStr="Pla and Molina (2004)" startWordPosition="2353" endWordPosition="2356">kin to previous work on splitting labels in order to obtain better statistics (e.g., Brants (1996); Ule (2003)) for situations with “the same label but different usage” (Ule, 2003, p. 181). By taking this approach, we are narrowing in on what annotators were instructed to focus on, namely “difficult tagging decisions,” (Santorini, 1990, p. 7). We implement this idea by assigning words a new, complex tag composed of its ambiguity class and the benchmark tag for that position. For example, ago has the ambiguity class IN/RB, and in example (5a), it resolves to RB. Thus, following the notation in Pla and Molina (2004), we assign ago the complex ambiguity tag &lt;IN/RB,RB&gt; in the training data, as shown in (5b). (5) a. ago/RB b. ago/&lt;IN/RB,RB&gt; Complex ambiguity tags can provide better distinctions than the unaltered tags. For example, words which vary between IN and RB and tagged as IN (e.g., ago, tagged &lt;IN/RB,IN&gt;) can ignore the contextual information that words varying between DT (determiner) and IN (e.g., that, tagged &lt;DT/IN,IN&gt;) provide. This proposal is in the spirit of a tagger like that described in Marquez et al (2000), which breaks the POS tagging problem into one problem for each ambiguity class, bu</context>
</contexts>
<marker>Pla, Molina, 2004</marker>
<rawString>Ferran Pla and Antonio Molina. 2004. Improving partof-speech tagging using lexicalized HMMs. Natural Language Engineering, 10(2):167–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank project (3rd revision, 2nd printing).</title>
<date>1990</date>
<tech>Technical Report MS-CIS-90-47,</tech>
<institution>The University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="6639" citStr="Santorini, 1990" startWordPosition="1067" endWordPosition="1068">gh to experiment with error correction. 3 Methodology Since we intend to correct a corpus with POS annotation errors, we have no true benchmark by which to gauge the accuracy of the corrected corpus, and we thus created a hand-checked subcorpus. Using the variation n-gram output, we flagged every non-fringe variation nucleus (token) as a potential error, giving us 21,575 flagged positions in the WSJ. From this set, we sampled 300 positions, removed the tag for each position, and hand-marked what the correct tag should be, based solely on the tagset definitions given in the WSJ tagging manual (Santorini, 1990), i.e., blind to the original data. Because some of the tagset distinctions were not defined clearly enough in the guidelines, in 20 cases we could not decide what the exact tag should be. For the purposes of comparison, we score a match with either tag as correct since a human could not disambiguate such cases. For the benchmark, we find that 201 positions in our sample set of 300 are correct, giving us a precision of 67%. A correction method must then surpass this precision figure in order to be useful. 4 Approach to correction Since our error detection phase relies on variation in annotatio</context>
<context position="11244" citStr="Santorini, 1990" startWordPosition="1838" endWordPosition="1839">ides enough context to disambiguate. TnT uses a fixed context of trigrams, and so can be swayed by irrelevant tags—here, the previous tags—which DTT can in principle ignore.6 (2) Mr. Bush was n’t interested in such/JJ an informal get-together. 5 Modifying the tagging model The errors detected by the variation n-gram method arise from variation in the corpus, often reflecting decisions difficult for annotators to maintain over the entire corpus, for example, the distinction between preposition (IN) and particle (RP) (as in (1)). Although these distinctions are listed in the tagging guidelines (Santorini, 1990), nowhere are they encoded in the tags themselves; thus, a tagger has no direct way of knowing that IN and RP are easily confusable but IN and NN (common noun) are not. In order to improve automatic correction, we can add information about these recurring distinctions to the tagging model, making the tagger aware of the difficult distinctions. But how do we make a tagger “aware” of a relevant problematic distinction? Consider the domain of POS tagging. Every word patterns uniquely, yet there are generalizations about words which we capture by grouping them into POS classes. By grouping words i</context>
<context position="13931" citStr="Santorini, 1990" startWordPosition="2304" endWordPosition="2305"> delineated, this example demonstrates that such classes can be used to redefine a tagging model with more unified groupings. 5.1 Using complex ambiguity tags We thus propose splitting a class such as RB into subclasses, using these ambiguity classes—JJ/RB, NN/RB, IN/RB, etc.—akin to previous work on splitting labels in order to obtain better statistics (e.g., Brants (1996); Ule (2003)) for situations with “the same label but different usage” (Ule, 2003, p. 181). By taking this approach, we are narrowing in on what annotators were instructed to focus on, namely “difficult tagging decisions,” (Santorini, 1990, p. 7). We implement this idea by assigning words a new, complex tag composed of its ambiguity class and the benchmark tag for that position. For example, ago has the ambiguity class IN/RB, and in example (5a), it resolves to RB. Thus, following the notation in Pla and Molina (2004), we assign ago the complex ambiguity tag &lt;IN/RB,RB&gt; in the training data, as shown in (5b). (5) a. ago/RB b. ago/&lt;IN/RB,RB&gt; Complex ambiguity tags can provide better distinctions than the unaltered tags. For example, words which vary between IN and RB and tagged as IN (e.g., ago, tagged &lt;IN/RB,IN&gt;) can ignore the </context>
<context position="15218" citStr="Santorini, 1990" startWordPosition="2529" endWordPosition="2530"> (e.g., that, tagged &lt;DT/IN,IN&gt;) provide. This proposal is in the spirit of a tagger like that described in Marquez et al (2000), which breaks the POS tagging problem into one problem for each ambiguity class, but because we alter the tagset here, different underlying tagging algorithms can be used. To take an example, consider the 5-gram revenue of about $ 370 as it is tagged by TnT. The 5-gram (at position 1344) in the WSJ is annotated as in (6). The tag for about is incorrect since “about when used to mean ’approximately’ should be tagged as an adverb (RB), rather than a preposition (IN)” (Santorini, 1990, p. 22). (6) revenue/NN of/IN about/IN $/$ 370/CD Between of and $, the word about varies between preposition (IN) and adverb (RB): it is IN 67 times and RB 65 times. After training TnT on the original corpus, we find that RB is a slightly better predictor of the following $ tag, as shown in (7), but, due to the surrounding probabilities, IN is the tag TnT assigns. (7) a. p($|IN,RB) = .0859 b. p($|IN,IN) = .0635 The difference between probabilities is more pronounced in the model with complex ambiguity tags. The word about generally varies between three tags: IN, RB, and RP (particle), receiv</context>
<context position="27234" citStr="Santorini, 1990" startWordPosition="4593" endWordPosition="4594">on that proposed should be VBN is that it indicates a specific event. Since our method uses no external semantic information, we have no way to know how to correct this.8 Other distinctions, such as the one between VBD and VBN, require some form of non-local knowledge in order to disambiguate because it depends on the presence or absence of an auxiliary verb, which can be arbitrarily far away. Secondly, sometimes the corpus was more often wrong than right for a particular pattern. This can be illustrated by looking at the word later in example (11), from the WSJ corpus. In the tagging manual (Santorini, 1990, p. 25), we find the description of later as in (12). (11) Now, 13 years later, Mr. Lane has revived his Artist ... (12) later should be tagged as a simple adverb (RB) rather than as a comparative adverb (RBR), unless its meaning is clearly comparative. A 8Note that it could be argued that this lack of a structural distinction contributed to the inconsistency among annotators in the first place and thus made error detection successful. useful diagnostic is that the comparative later can be preceded by even or still. In example (11), along with the fact that this is 13 years later as compared </context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini. 1990. Part-of-speech tagging guidelines for the Penn Treebank project (3rd revision, 2nd printing). Technical Report MS-CIS-90-47, The University of Pennsylvania, Philadelphia, PA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1997</date>
<booktitle>New Methods in Language Processing,</booktitle>
<pages>154--164</pages>
<editor>In D.H. Jones and H.L. Somers, editors,</editor>
<publisher>UCL Press,</publisher>
<location>London.</location>
<contexts>
<context position="8760" citStr="Schmid, 1997" startWordPosition="1437" endWordPosition="1438">eralize these patterns to the problematic parts of the corpus. This approach hinges on high-quality error detection since in general we cannot assume that discrepancies between a POS tagger and the benchmark are errors in the benchmark. Van Halteren (2000), for example, found that his tagger was correct in only 20% of disagreements with the benchmark. By focusing only on the variationflagged positions, we expect the tagger decisions to be more often correct than incorrect. We use two off-the-shelf taggers for correction, the Markov model tagger TnT (Brants, 2000) and the Decision Tree Tagger (Schmid, 1997), which we will abbreviate as DTT. Both taggers use probabilistic contextual and lexical information to disambiguate a tag at a particular corpus position. The difference is that TnT obtains contextual probabilities from maximum likelihood counts, whereas DTT constructs binary-branching decision trees to obtain contextual probabilities. In both cases, instead of looking at n-grams of words, the taggers use n-grams of tags. This generalization is desirable, as the variation n-gram method shows that the corpus has conflicting labels for the exact same sequence of n words. Results For the TnT tag</context>
<context position="19045" citStr="Schmid (1997)" startWordPosition="3177" endWordPosition="3178">ord’s ambiguity tag also appears as a variation ambiguity; or (b) a simple tag, otherwise. Variation words (choice 1) We start with variation nuclei because these are the potential errors we wish to correct. An example of choice 1a is ago, which varies between IN and RB as a nucleus, and so receives the tag &lt;IN/RB,IN&gt; when it resolves to IN and &lt;IN/RB,RB&gt; when it resolves to RB. The choices are based on relevance, though; instead of simply assigning all tags occurring in an ambiguity to an ambiguity class, we filter out ambiguities which we deem irrelevant. Similar to Brill and Pop (1999) and Schmid (1997), we do this by examining the variation unigrams and removing tags which occur less than 0.01 of the time for a word and less than 10 times overall. This eliminates variations like ,/DT where DT appears 4210 times for an, but the comma tag appears only once. Doing this means that an can now be grouped with other unambiguous determiners (DT). In addition to removing some erroneous classes, we gain generality and avoid data sparseness by using fewer ambiguity classes. This pruning also means that some variation words will receive tags which are not part of a variation, which is when choice 1b is</context>
</contexts>
<marker>Schmid, 1997</marker>
<rawString>Helmut Schmid. 1997. Probabilistic part-of-speech tagging using decision trees. In D.H. Jones and H.L. Somers, editors, New Methods in Language Processing, pages 154–164. UCL Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tylman Ule</author>
</authors>
<title>Directed treebank refinement for PCFG parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of TLT 2003,</booktitle>
<pages>177--188</pages>
<location>V¨axj¨o, Sweden.</location>
<contexts>
<context position="13704" citStr="Ule (2003)" startWordPosition="2269" endWordPosition="2270"> people think 0 I will give away/RP the store (4) a. Saturday ’s crash ... that *T* killed 132 of the 146 people aboard/RB b. These are used * aboard/IN military helicopters Although not every ambiguity class is so cleanly delineated, this example demonstrates that such classes can be used to redefine a tagging model with more unified groupings. 5.1 Using complex ambiguity tags We thus propose splitting a class such as RB into subclasses, using these ambiguity classes—JJ/RB, NN/RB, IN/RB, etc.—akin to previous work on splitting labels in order to obtain better statistics (e.g., Brants (1996); Ule (2003)) for situations with “the same label but different usage” (Ule, 2003, p. 181). By taking this approach, we are narrowing in on what annotators were instructed to focus on, namely “difficult tagging decisions,” (Santorini, 1990, p. 7). We implement this idea by assigning words a new, complex tag composed of its ambiguity class and the benchmark tag for that position. For example, ago has the ambiguity class IN/RB, and in example (5a), it resolves to RB. Thus, following the notation in Pla and Molina (2004), we assign ago the complex ambiguity tag &lt;IN/RB,RB&gt; in the training data, as shown in (5</context>
</contexts>
<marker>Ule, 2003</marker>
<rawString>Tylman Ule. 2003. Directed treebank refinement for PCFG parsing. In Proceedings of TLT 2003, pages 177–188, V¨axj¨o, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>The detection of inconsistency in manually tagged text.</title>
<date>2000</date>
<booktitle>In Anne Abeill´e, Thosten Brants, and Hans Uszkoreit, editors, Proceedings of LINC-00,</booktitle>
<location>Luxembourg.</location>
<marker>van Halteren, 2000</marker>
<rawString>Hans van Halteren. 2000. The detection of inconsistency in manually tagged text. In Anne Abeill´e, Thosten Brants, and Hans Uszkoreit, editors, Proceedings of LINC-00, Luxembourg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>